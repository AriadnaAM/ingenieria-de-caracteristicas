**1.Importa las librerías requeridas.**

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA

**2.Lee el archivo CSV llamado empleadosRETO.csv y coloca los datos en un frame de Pandas llamado EmpleadosAttrition.**



from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)

EmpleadosAttrition=pd.read_csv('/content/gdrive/MyDrive/empleadosRETO.csv')

**3.	Elimina las columnas que, con alta probabilidad, no tienen relación alguna con la salida. Hay algunas columnas que contienen información que no ayuda a definir el desgaste de un empleado**

EmpleadosAttrition = EmpleadosAttrition.drop(columns=['EmployeeCount', 'EmployeeNumber','Over18','StandardHours'], axis=1)

**4.	Analizando la información proporcionada, detectaste que no se cuenta con los años que el empelado lleva en la compañía y parece ser un buen dato. Dicha cantidad se puede calcular con la fecha de contratación ‘HiringDate’:**

*a.	Crea una columna llamada Year y obtén el año de contratación del empleado a partir de su fecha ‘HiringDate’. No se te olvide que debe ser un entero*

EmpleadosAttrition['Year'] = EmpleadosAttrition['HiringDate'].str[-4:].astype(int)

*b.	Crea una columna llamada YearsAtCompany que contenga los años que el empleado lleva en la compañía hasta el año 2018. Para su cálculo, usa la variable Year que acabas de crear.*

EmpleadosAttrition['YearsAtCompany']=2018-EmpleadosAttrition['Year']

**5.	La DistanceFromHome está dada en kilómetros, pero tiene las letras “km” al final y así no puede ser entera:**

*a.	Renombra la variable DistanceFromHome a DistanceFromHome_km.*


EmpleadosAttrition.rename(columns = {'DistanceFromHome': 'DistanceFromHome_km'}, inplace = True) 

*b.	Crea una nueva variable DistanceFromHome que sea entera, es decir, solo con números.*

EmpleadosAttrition['DistanceFromHome'] =EmpleadosAttrition['DistanceFromHome_km'].str.split(pat=' ').str[0].astype(int)

**6.	Borra las columnas Year, HiringDate y DistanceFromHome_km debido a que ya no son útiles.**

EmpleadosAttrition = EmpleadosAttrition.drop(columns=['Year', 'HiringDate','DistanceFromHome_km'], axis=1)

**7.	Aprovechando los ajustes que se están haciendo, la empresa desea saber si todos los departamentos tienen un ingreso promedio similar. Genera una nuevo frame llamado SueldoPromedioDepto que contenga el MonthlyIncome promedio por departamento de los empleados y colócalo en una variable llamada SueldoPromedio. Esta tabla solo es informativa, no la vas a utilizar en el set de datos que estás construyendo.**

#continents_group = continents_2019.groupby(['Continent']).mean()
SueldoPromedioDepto=EmpleadosAttrition[['Department','MonthlyIncome']]
SueldoPromedioDepto=SueldoPromedioDepto.groupby(['Department']).mean('MontlyIncome')

SueldoPromedioDepto.rename(columns={'MonthlyIncome':'SueldoPromedio'}, inplace = True)

SueldoPromedioDepto

**8.	La variable MonthlyIncome tiene un valor numérico muy grande comparada con las otras variables. Escala dicha variable para que tenga un valor entre 0 y 1.**

EmpleadosAttrition['MonthlyIncome']=(EmpleadosAttrition['MonthlyIncome']-EmpleadosAttrition['MonthlyIncome'].min())/(EmpleadosAttrition['MonthlyIncome'].max()-EmpleadosAttrition['MonthlyIncome'].min())

**9.	Todo parece indicar que las variables categóricas que quedan sí son importantes para obtener la variable de salida. Convierte todas las variables categóricas que quedan a numéricas:**

BusinessTravel_OH = pd.get_dummies(EmpleadosAttrition.BusinessTravel, prefix = 'classTravel')
Department_OH = pd.get_dummies(EmpleadosAttrition.Department, prefix = 'classDepartment')
EducationField_OH = pd.get_dummies(EmpleadosAttrition.EducationField, prefix = 'classEducation')
Gender_OH = pd.get_dummies(EmpleadosAttrition.Gender, prefix = 'classGender')
JobRole_OH = pd.get_dummies(EmpleadosAttrition.JobRole, prefix = 'classRole')
MaritalStatus_OH = pd.get_dummies(EmpleadosAttrition.MaritalStatus, prefix = 'classMaritalStatus')
Attrition_OH = pd.get_dummies(EmpleadosAttrition.Attrition, prefix = 'classAttrition')
EmpleadosAttrition = pd.concat([EmpleadosAttrition, BusinessTravel_OH,Department_OH,Gender_OH,JobRole_OH,MaritalStatus_OH,Attrition_OH], axis=1)

EmpleadosAttrition = EmpleadosAttrition.drop(columns=['Department','BusinessTravel','EducationField','Gender','JobRole','MaritalStatus','Attrition'], axis=1)

**10.	Ahora debes hacer la evaluación de las variables para quedarte con las mejores. Calcula la correlación lineal de cada una de las variables con respecto al Attrition.**

correlations = EmpleadosAttrition.corr()['classAttrition_Yes'].abs()

correlations

**11.	Selecciona solo aquellas variables que tengan una correlación mayor o igual a 0.1, dejándolas en otro frame llamado EmpleadosAttritionFinal. No olvides mantener la variable de salida Attrition; esto es equivalente a borrar las que no cumplen con el límite.**

variables_seleccion= correlations[correlations >= 0.1].index.tolist()
variables_seleccion

EmpleadosAttritionFinal = EmpleadosAttrition[EmpleadosAttrition.columns[EmpleadosAttrition.columns.isin(variables_seleccion)]]
EmpleadosAttritionFinal

**12.	Crea una nueva variable llamada EmpleadosAttritionPCA formada por los componentes principales del frame EmpleadosAttritionFinal. Recuerda que el resultado del proceso PCA es un numpy array, por lo que, para hacer referencia a una columna, por ejemplo, la 0, puedes usar la instrucción EmpleadosAttritionPCA[:,0]).**

pca=PCA()
features = EmpleadosAttritionFinal.drop(columns=['classAttrition_No','classAttrition_Yes'])
std_features = (features - features.mean()) / features.std()
EmpleadosAttritionPCA_array = pca.fit_transform(std_features)
EmpleadosAttritionPCA = pd.DataFrame(data=EmpleadosAttritionPCA_array, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])

EmpleadosAttritionPCA_array

EmpleadosAttritionPCA

**13.	Agrega el mínimo número de Componentes Principales en columnas del frame EmpleadosAttritionPCA que logren explicar el 80% de la varianza, al frame EmpleadosAttritionFinal. Puedes usar la instrucción assign, columna por columna, llamando a cada una C0, C1, etc., hasta las que vayas a agregar.**

cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)
num_components = np.argmax(cumulative_var_ratio >= 0.8) + 1
EmpleadosAttritionPCA = pd.DataFrame(data=EmpleadosAttritionPCA_array[:, :num_components],
                                      columns=[f'C{i}' for i in range(num_components)])
EmpleadosAttritionFinal = EmpleadosAttritionFinal.assign(**EmpleadosAttritionPCA)
EmpleadosAttritionFinal

EmpleadosAttritionFinal.to_csv('/content/gdrive/MyDrive/EmpleadosAttritionFinal.csv', index=False)
